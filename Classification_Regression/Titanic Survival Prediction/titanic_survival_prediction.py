# -*- coding: utf-8 -*-
"""Titanic_Survival_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/shravan-18/Machine-Learning-Projects/blob/main/Classification_Regression/Titanic%20Survival%20Prediction/Titanic_Survival_Prediction.ipynb
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

pip install --index-url=https://pypi.org/simple/ pycaret

import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from pycaret.regression import *

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.svm import SVR
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier

def Preprocess_Pipeline(df):

    df = df.drop(["Name", "Ticket", "Cabin"], axis=1)
    Sex_OneHotEncoded = pd.get_dummies(df["Sex"], drop_first=True)
    df = df.drop("Sex", axis=1)
    df["Sex"] = Sex_OneHotEncoded


    Embarked_OneHotEncoded = pd.get_dummies(df["Embarked"], drop_first = True)
    df = df.drop("Embarked", axis=1)
    df = pd.concat([df, Embarked_OneHotEncoded], axis = 1)


    # Calculate the correlation matrix
    correlation_matrix = df.corr()
    # Set the threshold for correlation
    threshold = 0.9
    # Find highly correlated columns
    highly_correlated_cols = set()
    cols = correlation_matrix.columns
    for i in range(len(cols)):
        for j in range(i):
            if correlation_matrix.iloc[i, j] >= threshold:
                colname = correlation_matrix.columns[i]
                highly_correlated_cols.add(colname)
    # Drop highly correlated columns
    df = df.drop(columns=highly_correlated_cols)


    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
    # Columns to impute
    columns_to_impute = ['Age', 'Fare']
    # Fitting the imputer on the data and transforming it
    transformed = imputer.fit_transform(df[columns_to_impute])
    df = df.drop(["Age", "Fare"], axis = 1)
    df = pd.concat([df, pd.DataFrame(transformed, columns = columns_to_impute)], axis = 1)


    return df

dataset = pd.read_csv("/content/train.csv")
dataset.head()

dataset.describe()

dataset.columns

print(dataset.shape)
print(dataset.isnull().sum())

dataset = dataset.drop(["Name", "Ticket", "Cabin"], axis=1)

Sex_OneHotEncoded = pd.get_dummies(dataset["Sex"], drop_first=True)
dataset.drop("Sex", axis=1)
dataset["Sex"] = Sex_OneHotEncoded

Embarked_OneHotEncoded = pd.get_dummies(dataset["Embarked"], drop_first = True)
dataset = dataset.drop("Embarked", axis=1)
dataset = pd.concat([dataset, Embarked_OneHotEncoded], axis = 1)

dataset.head()

sns.heatmap(dataset.corr(), cmap="YlGnBu")
plt.show()

dataset.isnull().sum()

dataset.shape

imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
imp_mean.fit(dataset[['Age']])
SimpleImputer()
transformed_age = imp_mean.transform(dataset[['Age']])

dataset = dataset.drop("Age", axis = 1)
dataset = pd.concat([dataset, pd.DataFrame(transformed_age, columns = ["Age"])], axis = 1)

dataset.head()

dataset.isnull().sum()

split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)
for train_indices, test_indices in split.split(dataset, dataset[["Survived", "Pclass", "Sex"]]):
    Train_data = dataset.loc[train_indices]
    Test_data = dataset.loc[test_indices]

plt.subplot(1, 2, 1)
Train_data['Survived'].hist()
Train_data['Sex'].hist()
Train_data['Pclass'].hist()
plt.title("Train Data")

plt.subplot(1, 2, 2)
Test_data['Survived'].hist()
Test_data['Sex'].hist()
Test_data['Pclass'].hist()
plt.title("Test Data")

plt.show()

Train_data.head()

Train_data.head()

Test_data.head()

X_train = Train_data.drop(["Survived", "PassengerId"], axis = 1)
y_train = Train_data["Survived"]

X_test = Test_data.drop(["Survived", "PassengerId"], axis = 1)
y_test = Test_data["Survived"]

temp = X_train['Fare']
X_train = X_train.drop('Fare', axis = 1)
X_train = pd.concat([X_train, temp], axis = 1)

temp = X_test['Fare']
X_test = X_test.drop('Fare', axis = 1)
X_test = pd.concat([X_test, temp], axis = 1)

print(X_train.shape, X_test.shape)
print(y_train.shape, y_test.shape)

# Creating and training the linear regression model
LRmodel = LinearSVC()
LRmodel.fit(X_train, y_train)

# Making predictions on the test set
y_pred = np.round(LRmodel.predict(X_test)).astype(int)

# Calculating the Accuracy
acc = accuracy_score(y_test, y_pred)

print(f"Accuracy: {acc}")

# Creating and training the XGBoost regression model
XGBmodel = XGBClassifier(n_estimators=100, max_depth=3, gamma = 0.15)
XGBmodel.fit(X_train, y_train)

# Making predictions on the test set
y_pred = (XGBmodel.predict(X_test))

# Calculating the Accuracy
acc = accuracy_score(y_test, y_pred)

print(f"Accuracy: {acc}")

# Creating and training the Decision Tree Regression model
DTRModel = DecisionTreeClassifier(criterion = 'entropy', max_depth=4)
DTRModel.fit(X_train, y_train)

# Making predictions on the test set
y_pred = (DTRModel.predict(X_test))

# Calculating the Accuracy
acc = accuracy_score(y_test, y_pred)

print(f"Accuracy: {acc}")

# Creating and training the Random Forest Regression model
RFRmodel = RandomForestClassifier(n_estimators=300)
RFRmodel.fit(X_train, y_train)

# Making predictions on the test set
y_pred = (RFRmodel.predict(X_test))

# Calculating the Accuracy
acc = accuracy_score(y_test, y_pred)

print(f"Accuracy: {acc}")

# Creating and training the Extra Trees Regression model
ETRmodel = ExtraTreesClassifier(n_estimators=200)
ETRmodel.fit(X_train, y_train)

# Making predictions on the test set
y_pred = ETRmodel.predict(X_test)

# Calculating the Accuracy
acc = accuracy_score(y_test, y_pred)

print(f"Accuracy: {acc}")

# Creating the K-Nearest Neighbors (KNN) classifier model
KNNmodel = KNeighborsClassifier(n_neighbors=5)  # Set the number of neighbors

# Training the model
KNNmodel.fit(X_train, y_train)

# Making predictions on the test set
y_pred = KNNmodel.predict(X_test)

# Calculating the accuracy
accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")

# Initialize the regression setup
regression_setup = setup(data=dataset, target='Survived', train_size=0.8)

# Compare different regression models
best_model = compare_models()

from sklearn.ensemble import GradientBoostingClassifier

# Creating and training the Gradient Boosting Regression model
GBRmodel = GradientBoostingClassifier(n_estimators=100, random_state=0)
GBRmodel.fit(X_train, y_train.ravel())  # ravel y_train to remove extra dimensions

# Making predictions on the test set
y_pred = GBRmodel.predict(X_test)

# Calculating the Accuracy
acc = accuracy_score(y_test, y_pred)

print(f"Accuracy: {acc}")

# Creating the ANN model
ANNmodel = keras.Sequential([
    layers.Input(shape=X_train.shape[1]),  # Input layer with 10 features
    layers.Dense(100, activation='relu'),  # Hidden layer with 16 neurons and ReLU activation
    layers.Dense(10, activation='relu'),   # Hidden layer with 8 neurons and ReLU activation
    layers.Dense(1, activation = 'sigmoid')  # Output layer with sigmoid activation for binary classification
])

# Compiling the model
ANNmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Training the model
ANNmodel.fit(X_train, y_train, epochs=20, validation_data = (X_test, y_test))

# Evaluating the model on the test set
ANN_preds = ANNmodel.evaluate(X_test, y_test)

# # Create a regression model using the best-performing algorithm
# final_model = create_model(best_model)

# # Make predictions on the test set
# predictions = predict_model(final_model)
# predictions

test = pd.read_csv("/content/test.csv")
test = Preprocess_Pipeline(test)

test.head()

test_preds = DTRModel.predict(test.drop("PassengerId", axis = 1))
test_preds

sub = pd.read_csv('/content/gender_submission.csv')
sub.head()

Submission = pd.concat([test["PassengerId"], pd.DataFrame(test_preds)], axis = 1)
Submission.columns = ["PassengerId", "Survived"]
Submission.head()

# Specify the file path to delete
file_path = '/content/submissions.csv'  # Replace with the actual file path

# Check if the file exists before attempting to delete
if os.path.exists(file_path):
    os.remove(file_path)
    print(f"File '{file_path}' deleted successfully.")
else:
    print(f"File '{file_path}' does not exist.")

# Save the DataFrame to a CSV file
csv_file_path = '/content/submissions.csv'
Submission.to_csv(csv_file_path, index=False)

pd.read_csv("/content/submissions.csv").head()