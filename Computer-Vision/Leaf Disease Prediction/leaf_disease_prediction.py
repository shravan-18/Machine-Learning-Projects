# -*- coding: utf-8 -*-
"""leaf-disease-prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/shravan-18/Machine-Learning-Projects/blob/main/Computer-Vision/Leaf%20Disease%20Prediction/leaf-disease-prediction.ipynb
"""

# # This Python 3 environment comes with many helpful analytics libraries installed
# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# # For example, here's several helpful packages to load

# import numpy as np # linear algebra
# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# # Input data files are available in the read-only "../input/" directory
# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import os
import shutil
import pandas as pd

# Load the CSV file
df = pd.read_csv('/kaggle/input/leaf-disease/train.csv')

# Get the unique labels
labels = df['Label'].unique()

# Create a directory for each label
for label in labels:
    os.makedirs(os.path.join('/kaggle/working/', str(label)), exist_ok=True)

# Loop over the rows in the DataFrame
for index, row in df.iterrows():
    # Get the image name and its corresponding label
    image_name = row['Image']
    label = str(row['Label'])

    # Construct the source and destination paths
    source = os.path.join('/kaggle/input/leaf-disease/train', image_name)
    destination = os.path.join('/kaggle/working/', label, image_name)

    # Move the image
    shutil.copy(source, destination)
#     print(source)
#     print(destination)
#     print()

from sklearn.model_selection import train_test_split

# Specify the directory where your images are currently stored
source_dir = '/kaggle/working/'

# Specify the directories where you want to create the train and validation sets
train_dir = '/kaggle/working/train'
val_dir = '/kaggle/working/val'

# Get the list of all labels
labels = [label for label in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, label))]

# Loop over the labels
for label in labels:
    # Get the list of all images for this label
    images = os.listdir(os.path.join(source_dir, label))

    # Check if the list of images is empty or contains only one image
    if len(images) < 2:
        print(f"Found less than 2 images for label {label}")

        if (len(images)!=0):
            for image in images:
                source = os.path.join(source_dir, label, image)
                destination = os.path.join(train_dir, label, image)
                shutil.move(source, destination)

            continue
        else:
            continue

    # Split the images into a training set and a validation set
    train_images, val_images = train_test_split(images, test_size=0.2, random_state=42)

    # Create directories for this label in the train and validation directories
    os.makedirs(os.path.join(train_dir, label), exist_ok=True)
    os.makedirs(os.path.join(val_dir, label), exist_ok=True)

    # Move the training images to the train directory
    for image in train_images:
        source = os.path.join(source_dir, label, image)
        destination = os.path.join(train_dir, label, image)
        shutil.move(source, destination)

    # Move the validation images to the validation directory
    for image in val_images:
        source = os.path.join(source_dir, label, image)
        destination = os.path.join(val_dir, label, image)
        shutil.move(source, destination)

train_dir = '/kaggle/working/train'
val_dir = '/kaggle/working/val'

import tensorflow as tf

IMAGE_SIZE = (224, 224)
BATCH_SIZE = 32

train_data = tf.keras.preprocessing.image_dataset_from_directory(train_dir,
                                                                           image_size=IMAGE_SIZE,
                                                                           batch_size=BATCH_SIZE,
                                                                           label_mode="categorical")

valid_data = tf.keras.preprocessing.image_dataset_from_directory(val_dir,
                                                                           image_size=IMAGE_SIZE,
                                                                           batch_size=BATCH_SIZE,
                                                                           label_mode="categorical")

import matplotlib.pyplot as plt
import numpy as np

class_labels = train_data.class_names
class_counts = np.zeros(len(class_labels))

for images, labels in train_data:
    label_indices = np.argmax(labels, axis=1)
    for label in label_indices:
        class_counts[label] += 1

plt.bar(class_labels, class_counts)
plt.xlabel('Class label')
plt.ylabel('Number of samples')
plt.title('Class distribution in dataset')
plt.show()

import tensorflow as tf

strategy = tf.distribute.MirroredStrategy()
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))

from tensorflow.keras import layers as L
from tensorflow.keras.models import Model

def Create_Model(Input_shape):

    Inputs = L.Input(shape=Input_shape)

    x = L.Conv2D(64, 3, padding="same")(Inputs)
    x = L.MaxPooling2D((2,2))(x)
    x = L.Conv2D(128, 3, padding="same")(x)
    x = L.MaxPooling2D((2,2))(x)
    x = L.Conv2D(256, 3, padding="same")(x)
    x = L.MaxPooling2D((2,2))(x)
    x = L.Conv2D(512, 3, padding="same")(x)
    x = L.MaxPooling2D((2,2))(x)
    x = L.Flatten()(x)

    x = L.Dense(128, activation="relu")(x)
    Outputs = L.Dense(10, activation="softmax")(x)

    model = Model(Inputs, Outputs)
    return model

len(train_data), len(valid_data)

model_1 = Create_Model((224, 224, 3))

print(model_1.input_shape)
print(model_1.output_shape)
print()
print(model_1.summary())

model_1.compile(loss="categorical_crossentropy",
               optimizer=tf.keras.optimizers.Adam(lr=0.0005),
               metrics=["accuracy"])

history_1 = model_1.fit(train_data,
           epochs = 20,
           batch_size=32,
           steps_per_epoch = len(train_data),
           validation_data = valid_data,
           validation_steps = int(0.25*len(valid_data)))

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py

from helper_functions import plot_loss_curves

plot_loss_curves(history_1)

a

test_preds = model_1.predict(images)

len(test_preds)

test_preds.shape

x = np.argmax(test_preds, axis=1)

res=pd.DataFrame({'Label': x})
res.to_csv("/kaggle/working/sub.csv",index=False)