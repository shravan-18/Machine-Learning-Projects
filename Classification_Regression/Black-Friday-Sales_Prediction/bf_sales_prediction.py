# -*- coding: utf-8 -*-
"""BF_Sales_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Af2XY-JyhOlHH1Y41D5RrMdJFrZs8xG4

# **Black Friday Sales Prediction**

## **Import Libraries**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split

"""## **Data Exploration and Analysis**"""

train = pd.read_csv("D:\Datasets\\train\\train.csv")
train.head()

# Check shape of data
print(f"Number of rows: {train.shape[0]}")
print(f"Number of columns: {train.shape[1]}")

# Check  datatypes of features
train.dtypes

# Check number of unique values in each column
for column in train.columns:
    print(f"{column}: {len(train[column].unique())}")

# Check NULL values in the data
train.isnull().sum()

train.describe()

# Check Correlation among columns
# Calculate the correlation matrix

correlation_matrix = train.corr().abs()

# Plot the correlation heatmap
plt.figure(figsize=(5, 5))
sns.heatmap(correlation_matrix, cmap='Greys',  annot=False)
plt.title('Correlation Heatmap')
plt.tight_layout()
plt.show()

# Determine the number of rows and columns for the grid
num_rows = 7
num_cols = 3

# Create a figure and a grid of subplots
fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 20))

# Flatten the axes array to loop through all subplots
axes = axes.flatten()

# Plot histograms for each column
for i, col in enumerate(train.columns):
    ax = axes[i]
    ax.hist(train[col], bins=20, color='blue', alpha=0.7)
    ax.set_title(col)
    ax.set_xlabel('Value')
    ax.set_ylabel('Frequency')

# Hide any remaining empty subplots
for i in range(len(train.columns), num_rows * num_cols):
    fig.delaxes(axes[i])

# Adjust layout to prevent subplot overlap
plt.tight_layout()

# Show the plots
plt.show()

"""## **Create Preprocessing Pipeline**"""

def preprocess_train(df):
    df = df.drop_duplicates() # Drop duplicate rows

    # One Hot Encoding
    gender = pd.get_dummies(df["Gender"], drop_first=True)
    city_cat = pd.get_dummies(df["City_Category"], drop_first=True)
    marital_status = pd.get_dummies(df["Marital_Status"], drop_first=True)
    age = pd.get_dummies(df["Age"], drop_first=True)
    stay_curr_city = pd.get_dummies(df["Stay_In_Current_City_Years"], drop_first=True)

    # Drop One hot encoded columns and concat with existing dataframe
    df = df.drop(["Gender", "City_Category", "Marital_Status", "Age", "Stay_In_Current_City_Years"], axis=1)
    df = pd.concat([df, gender, city_cat, marital_status, age, stay_curr_city], axis=1)

    # Label Encoding
    le = LabelEncoder()
    df['User_ID'] = le.fit_transform(df['User_ID'])
    le = LabelEncoder()
    df['Product_ID'] = le.fit_transform(df['Product_ID'])
    le = LabelEncoder()
    df['Product_Category_1'] = le.fit_transform(df['Product_Category_1'])

    df = df.drop(['Product_Category_2', 'Product_Category_3'], axis=1) # Drop columns with majority NULL values

    df.columns = df.columns.astype(str) # Convert column names to strings
    df.columns.values[15]='1_2'

    X_train, X_test, y_train, y_test = train_test_split(df.drop("Purchase", axis=1), df["Purchase"], test_size=0.25, random_state=69)

    scaler = MinMaxScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    return X_train, X_test, y_train, y_test, df

X_train, X_test, y_train, y_test, train = preprocess_train(train)

"""## **Model Training**"""

import pycaret

from pycaret.regression import *
exp_name = setup(data = train,  target = 'Purchase', use_gpu=True)
best_model = compare_models()