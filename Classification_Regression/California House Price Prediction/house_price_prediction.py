# -*- coding: utf-8 -*-
"""house-price-prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/shravan-18/Machine-Learning-Projects/blob/main/Classification_Regression/California%20House%20Price%20Prediction/house-price-prediction.ipynb

# California House Price Prediction.

## Introduction

This notebook explores and implements the housing price prediction using various regression machine learning models.

## Dataset

We use the Kaggle California Housing Prices Dataset, which pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data  - [Dataset Link](https://www.kaggle.com/datasets/camnugent/california-housing-prices)

# Import Basic Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_california_housing

"""# Opening the Train Dataset and Data Exploration"""

# Fetch the Dataset
var = fetch_california_housing()
var

# The data is as a dictionary. Let's check the keys in the dictionary
var.keys()

# Convert the features and target dictionaries to Pandas DataFrame
Data = pd.DataFrame(var['data'], columns= var['feature_names'])
Target = pd.DataFrame(var['target'], columns=['Price'])

# Concatenate the features and target columns into a single DataFrame
df = pd.concat([Data, Target], axis=1)
df.head()

# Create a copy for safety always
df_copy = df.copy()

# Check NULL Values, if any...
df.isnull().sum()

# Check the datatype of values in each column
for i in df.columns:
    print(i, type(df[i].iloc[0]))

"""# Data Preprocessing"""

# Plot a Heatmap of the columns, to check their correlations
plt.figure(figsize=(7,6))
seaborn.heatmap(df.corr().abs(), cmap="YlGnBu", annot=True)
plt.show()

upper_tri = df.corr().abs().where(np.triu(np.ones(df.corr().abs().shape),k=1).astype(np.bool))
print(upper_tri)

len(df.columns)

# Check the skewness of values in each column
for i in df.columns:
    print(i, df[i].skew())

# Drop the one amongst two columns, if there are any two columns which are highly correlated
'''
    Here we drop AveBedrms as AveBedrms and AveRooms are highly correlated
    Similarly, we drop Latitude as Latitude and Longitude are highly correlated
'''
df = df.drop('AveBedrms', axis=1)
df = df.drop('Latitude', axis=1)

# Plot the distribution of data in the columns
fig, axes = plt.subplots(3, 3, figsize=(12, 9))

# Flatten the axes for easier iteration
axes = axes.flatten()

# Plot each column on a separate subplot
for i, col in enumerate(df.columns):
    ax = axes[i]
    ax.hist(df[col], bins=20)
    ax.set_title(col)

# Adjust spacing between subplots
plt.tight_layout()

# Show the plot
plt.show()

# List the most skewed columns
skewed_cols = ['AveRooms', 'Population', 'AveOccup']

# Plot the skewed columns' distribution after taking log of the values to check if taking log will be a suitable method for handling data skewness
fig, axes = plt.subplots(2, 2, figsize=(6, 4))

# Flatten the axes for easier iteration
axes = axes.flatten()

# Plot each column on a separate subplot
for i, col in enumerate(skewed_cols):
    ax = axes[i]
    ax.hist(np.log(df[col]), bins=20)
    ax.set_title(col)

# Adjust spacing between subplots
plt.tight_layout()

# Show the plot
plt.show()

for i in skewed_cols:
    df[i] = np.log(df[i]) # Taking log as skewness is handled well, which can be inferred from the plot above

df['AveOccup'] = np.sqrt(df['AveOccup']) # Taking square root of the values here, as this proves to be a better approach over taking log of the values

# Plot the distributions of all columns after handling skewness
fig, axes = plt.subplots(3, 3, figsize=(12, 9))

# Flatten the axes for easier iteration
axes = axes.flatten()

# Plot each column on a separate subplot
for i, col in enumerate(df.columns):
    ax = axes[i]
    ax.hist(df[col], bins=20)
    ax.set_title(col)

# Adjust spacing between subplots
plt.tight_layout()

# Show the plot
plt.show()

# Check Skewness of each column again after handling it
for i in df.columns:
    print(i, df[i].skew())

df = df.drop_duplicates() # Drop duplicate entries

df.isna().sum() # Check for NULL/na Values - AveOccup has 3

# Fill in missing values for AVEOccup using Median
df['AveOccup'].fillna(df['AveOccup'].median(), inplace=True)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('Price', axis=1), df['Price'], test_size=0.20)

# Define a scaler function, which scales values between 0 and 1
scaler = MinMaxScaler()
# Fit and Transform values
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

"""# Use Machine Learning Models to Evaluate on the Data

## Pycaret is a very useful tool for checking multiple models' results at once.
"""

from pycaret.regression import *

# Initialize the regression setup
regression_setup = setup(data=df, target='Price', train_size=0.8)

# Compare different regression models
best_model = compare_models()

# Trying out CatBoost Regressor

from catboost import CatBoostRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Create and train the CatBoost Regressor model
model = CatBoostRegressor(iterations=500,  # Number of boosting iterations
                          depth=6,          # Depth of the trees
                          learning_rate=0.1, # Learning rate
                          loss_function='RMSE', # Loss function (Root Mean Squared Error)
                          verbose=100)      # Print progress every 200 iterations

model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

print()
# Evaluate the model
print(f"Mean Squared Error: {mean_squared_error(y_test, y_pred)}")
print(f"R2 Score: {r2_score(y_test, y_pred)}")

# Trying out XGB Regressor

import xgboost as xgb

# Create an XGBoost regression model
model = xgb.XGBRegressor(objective='reg:squarederror',  # For regression tasks
                         n_estimators=200,              # Number of boosting rounds (trees)
                         max_depth=3,                   # Maximum tree depth
                         learning_rate=0.1)             # Step size shrinkage during training

# Fit the model to the training data
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

print()
# Evaluate the model
print(f"Mean Squared Error: {mean_squared_error(y_test, y_pred)}")
print(f"R2 Score: {r2_score(y_test, y_pred)}")

"""### From the above results, we can see that the CatBoostRegressor and XGBoostRegressor have performed decently well, (CatBoostRegressor being slightly on the better end) in comparison to other models on the same data.

## Final Results

Best Model - CatBoostRegressor

Mean Squared Error: 0.37546097494384945

R2 Score: 0.7242200012905213
"""