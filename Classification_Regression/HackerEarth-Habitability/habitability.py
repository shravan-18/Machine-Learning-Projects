# -*- coding: utf-8 -*-
"""habitability.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/shravan-18/Machine-Learning-Projects/blob/main/Classification_Regression/HackerEarth-Habitability/habitability.ipynb

# **Hackathon by HackerEarth - House Habitability Prediction**

# **Import necessary libraries**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.impute import SimpleImputer
from pycaret.regression import *

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import HistGradientBoostingRegressor
from catboost import CatBoostRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.metrics import r2_score, mean_squared_error
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, f1_score, recall_score, precision_score, r2_score
import xgboost as xgb
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import StackingRegressor

"""# **Data Exploration and Analysis**"""

data = pd.read_csv("C:\Datasets\habitability\\train.csv")
data.head()

data.drop('Property_ID', axis=1, inplace=True)

data.isnull().sum()

def FillNA(df, cols_to_fill):
    for i in cols_to_fill:
        if type(data[i][0])==np.float64:
            df[i] = df[i].fillna(df[i].mean())
        else:
            most_frequent_value = df[i].mode()[0]
            df[i].fillna(most_frequent_value, inplace=True)

    return df

cols_to_fill = ['Number_of_Windows', 'Frequency_of_Powercuts', 'Crime_Rate', 'Furnishing', 'Dust_and_Noise']

data = FillNA(data, cols_to_fill)

len(data['Property_Type'].unique()), len(data['Furnishing'].unique()), len(data['Crime_Rate'].unique()), len(data['Dust_and_Noise'].unique()), len(data['Water_Supply'].unique()), len(data['Power_Backup'].unique())

data['Power_Backup'].unique()

data = pd.get_dummies(data,columns=['Property_Type'],drop_first=True)

## Handling Ordinal categorical features
data['Furnishing'].replace({'Unfurnished':1, 'Semi_Furnished':2, 'Fully Furnished':3}, inplace= True)

data['Power_Backup'].replace({'NOT MENTIONED':0, 'No':1, 'Yes':2}, inplace= True)

data['Water_Supply'].replace({'All time':3, 'Once in a day - Morning':2, 'Once in a day - Evening':2,'Once in two days':1,'NOT MENTIONED':0}, inplace= True)

data['Crime_Rate'].replace({'Well below average':4, 'Slightly below average':3, 'Slightly above average':2,'Well above average':1}, inplace= True)

data['Dust_and_Noise'].replace({'High':1, 'Medium':2, 'Low':3}, inplace= True)

data.head()

data.shape

plt.figure(figsize=(9,8))
seaborn.heatmap(data.corr(), cmap="YlGnBu", annot=False)
plt.show()

# Calculate the correlation matrix
correlation_matrix = data.corr()
# Set the threshold for correlation
threshold = 0.9
# Find highly correlated columns
highly_correlated_cols = set()
cols = correlation_matrix.columns
for i in range(len(cols)):
    for j in range(i):
        if correlation_matrix.iloc[i, j] >= threshold:
            colname = correlation_matrix.columns[i]
            highly_correlated_cols.add(colname)

highly_correlated_cols

data.isnull().sum()

data.describe()

c=1
plt.figure(figsize=(12, 10))

for i in data.columns:
    ax = plt.subplot(7, 4,c)
    c += 1
    seaborn.boxplot(data = data, y=i)

# def outliers(val):

#     if val>(q3+threshold):
#         return q3
#     elif val<(q1-threshold):
#         return q1

#     return val

# for i in data.columns:

#     q3 = data[i].describe()['75%']
#     q1 = data[i].describe()['25%']

#     iqr = q3-q1
#     threshold = iqr*1.5
#     data[i] = data[i].apply(outliers)

c=1
plt.figure(figsize=(12, 10))

for i in data.columns:
    ax = plt.subplot(7, 4,c)
    c += 1
    seaborn.boxplot(data = data, y=i)

data = data.drop_duplicates()
data.shape

scaler = MinMaxScaler()

c=1
plt.figure(figsize=(12, 10))

for i in data.columns:
    plt.subplot(7, 4, c)
    c += 1
    data[i].hist()
    plt.title(i)

plt.tight_layout()
plt.show()

"""# **Split Data into Train and Test partitions and scale for training**"""

X_train, X_test, y_train, y_test = train_test_split(data.drop('Habitability_score', axis=1), data['Habitability_score'], test_size=0.20)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

"""# **Make Predictions**"""

# mean_squared_error, f1_score, recall_score, precision_score, r2_score

LRmodel = LinearRegression()
LRmodel.fit(X_train, y_train)

# Making predictions on the test set
y_pred = LRmodel.predict(X_test)

print(f'R2 Score: {r2_score(y_test, y_pred)}')
print(f'MSE: {mean_squared_error(y_test, y_pred)}')

# Set the parameters for XGBoost
params = {
    'max_depth': 3,
    'n_estimators': 400,
}

# Train the XGBoost model
model = xgb.XGBRegressor(**params)
model.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = model.predict(X_test)

print(f'R2 Score: {r2_score(y_test, y_pred)}')
print(f'MSE: {mean_squared_error(y_test, y_pred)}')

# Creating and training the Decision Tree Regression model
DTRModel = DecisionTreeRegressor(max_depth=4)
DTRModel.fit(X_train, y_train)

# Making predictions on the test set
y_pred = (DTRModel.predict(X_test))

print(f'R2 Score: {r2_score(y_test, y_pred)}')
print(f'MSE: {mean_squared_error(y_test, y_pred)}')

# Creating and training the Random Forest Regression model
RFRmodel = RandomForestRegressor(n_estimators=300)
RFRmodel.fit(X_train, y_train)

# Making predictions on the test set
y_pred = (RFRmodel.predict(X_test))

# Calculating the Accuracy
print(f'R2 Score: {r2_score(y_test, y_pred)}')
print(f'MSE: {mean_squared_error(y_test, y_pred)}')

# Initialize the regression setup
regression_setup = setup(data=data, target='Habitability_score', train_size=0.8)

# Compare different regression models
best_model = compare_models()

xg=XGBRegressor(random_state=1,learning_rate=0.1)
rf=RandomForestRegressor(random_state=1)
ct=CatBoostRegressor(random_state=1)
hgb=HistGradientBoostingRegressor(random_state=1,l2_regularization=0.1)
lgbm = LGBMRegressor(random_state=1,learning_rate=0.1)

estimator_list = [
    ('ct',ct),
    ('rf',rf),
    ('xg',xg),
    ('lgbm',lgbm)
]

stack_model = StackingRegressor(
    estimators=estimator_list,final_estimator=HistGradientBoostingRegressor(random_state=1,l2_regularization=0.1)
)

# Train stacked model
stack_model.fit(X_train, y_train)

print("Train data score: ",stack_model.score(X_train,y_train))
print("Test data score: ",stack_model.score(X_test,y_test))

test = pd.read_csv("C:\Datasets\habitability\\test.csv")
ID = test['Property_ID']
test.drop('Property_ID', axis=1, inplace=True)
test.isnull().sum()

test.head()

test = FillNA(test, cols_to_fill)
test.isnull().sum()

test = pd.get_dummies(test,columns=['Property_Type'],drop_first=True)

## Handling Ordinal categorical features
test['Furnishing'].replace({'Unfurnished':1, 'Semi_Furnished':2, 'Fully Furnished':3}, inplace= True)

test['Power_Backup'].replace({'NOT MENTIONED':0, 'No':1, 'Yes':2}, inplace= True)

test['Water_Supply'].replace({'All time':3, 'Once in a day - Morning':2, 'Once in a day - Evening':2,'Once in two days':1,'NOT MENTIONED':0}, inplace= True)

test['Crime_Rate'].replace({'Well below average':4, 'Slightly below average':3, 'Slightly above average':2,'Well above average':1}, inplace= True)

test['Dust_and_Noise'].replace({'High':1, 'Medium':2, 'Low':3}, inplace= True)

# Calculate the correlation matrix
correlation_matrix = test.corr()
# Set the threshold for correlation
threshold = 0.9
# Find highly correlated columns
highly_correlated_cols = set()
cols = correlation_matrix.columns
for i in range(len(cols)):
    for j in range(i):
        if correlation_matrix.iloc[i, j] >= threshold:
            colname = correlation_matrix.columns[i]
            highly_correlated_cols.add(colname)

highly_correlated_cols

test = scaler.fit_transform(test)

test_preds = stack_model.predict(test)

test_preds

sub = pd.DataFrame({'Habitability_score':test_preds})

sub = pd.concat([ID, sub], axis=1)
sub

sub.to_csv('C:\VIT Material\VIT material\Projects\HackerEarth-Habitability\sub.csv', index=False)