{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/content/train\" # Replace with your training directory\n",
    "test_dir = \"/content/test\"  # Replace with your test directory\n",
    "\n",
    "class_names = os.listdir(train_dir)\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    validation_split=0.2,   \n",
    ")\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    directory=train_dir,\n",
    "    target_size=(150, 150),\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    seed=42,\n",
    "    subset=\"training\",\n",
    ")\n",
    "\n",
    "val_data = train_datagen.flow_from_directory(\n",
    "    directory=train_dir,\n",
    "    target_size=(150, 150),\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    seed=42,\n",
    "    subset=\"validation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "test_datagen = ImageDataGenerator(rescale=1./255,)\n",
    "\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    directory=test_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    seed=42,\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape = (150, 150, 3)):\n",
    "\n",
    "    Inputs = L.Input(shape=input_shape)\n",
    "    x = L.Conv2D(512, 3, padding=\"same\")(Inputs)\n",
    "    x = L.MaxPooling2D((2,2))(x)\n",
    "    x = L.Activation(\"relu\")(x)\n",
    "    x = L.Conv2D(256, 3, padding=\"same\")(Inputs)\n",
    "    x = L.MaxPooling2D((2,2))(x)\n",
    "    x = L.Activation(\"relu\")(x)\n",
    "    x = L.Conv2D(128, 3, padding=\"same\")(x)\n",
    "    x = L.MaxPooling2D((2,2))(x)\n",
    "    x = L.Activation(\"relu\")(x)\n",
    "    x = L.Conv2D(64, 3, padding=\"same\")(x)\n",
    "    x = L.MaxPooling2D((2,2))(x)\n",
    "    x = L.Activation(\"relu\")(x)\n",
    "    x = L.Flatten()(x)\n",
    "    x = L.Dense(256, activation=\"relu\")(x)\n",
    "    if num_classes > 2:\n",
    "        Outputs = L.Dense(num_classes, activation=\"softmax\")(x)\n",
    "        model = Model(Inputs, Outputs, name=\"Model\")\n",
    "        model.compile(\n",
    "            loss = \"categorical_crossentropy\",\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "    else:\n",
    "        Outputs = L.Dense(num_classes, activation=\"sigmoid\")(x)\n",
    "        model = Model(Inputs, Outputs, name=\"Model\")\n",
    "        model.compile(\n",
    "            loss = \"binary_crossentropy\",\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data,\n",
    "    epochs=32,\n",
    "    validation_data = val_data,\n",
    "    callbacks = [early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the validation and training data separately\n",
    "def plot_loss_curves(history):\n",
    "  \"\"\"\n",
    "  Returns separate loss curves for training and validation metrics.\n",
    "  \"\"\"\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  accuracy = history.history['accuracy']\n",
    "  val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "  epochs = range(len(history.history['loss']))\n",
    "\n",
    "  # Plot loss\n",
    "  plt.plot(epochs, loss, label='training_loss')\n",
    "  plt.plot(epochs, val_loss, label='val_loss')\n",
    "  plt.title('Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.legend()\n",
    "\n",
    "  # Plot accuracy\n",
    "  plt.figure()\n",
    "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
    "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
    "  plt.title('Accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store filenames and predicted classes\n",
    "filenames = []\n",
    "predictions = []\n",
    "\n",
    "# Iterate over the images in the folder\n",
    "for filename in os.listdir(test_dir):\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        # Load and preprocess the image\n",
    "        image_path = os.path.join(test_dir, filename)\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize((150, 150))\n",
    "        image = np.array(image)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image = image / 255.0\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = model.predict(image)\n",
    "        predicted_class = np.argmax(prediction)  # Get the index of the highest probability class\n",
    "\n",
    "        # Extract filename without extension\n",
    "        filename_no_ext = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Append to lists\n",
    "        filenames.append(filename_no_ext)\n",
    "        predictions.append(predicted_class)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {'ID': filenames, 'LABEL': predictions}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Write DataFrame to CSV\n",
    "df.to_csv(\"/content/Submission-Final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
