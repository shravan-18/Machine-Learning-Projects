{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2YA1vLiEUOl",
        "outputId": "8e40bae9-846d-4f62-a153-28af3d0408ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-05 16:04:59--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py.1’\n",
            "\n",
            "\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-09-05 16:04:59 (20.7 MB/s) - ‘helper_functions.py.1’ saved [10246/10246]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GF3BkJJEFo85",
        "outputId": "26dd6b94-6e1d-4a76-e703-b5f66eb677f3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.13.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys"
      ],
      "metadata": {
        "id": "xiR6TsqBE4_o"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "k3sJB0sjNRzj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get TensorFlow Datasets\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Get all available datasets in TFDS\n",
        "datasets_list = tfds.list_builders()\n",
        "\n",
        "# Set our target dataset and see if it exists\n",
        "target_dataset = \"food101\"\n",
        "print(f\"'{target_dataset}' in TensorFlow Datasets: {target_dataset in datasets_list}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmNC2os6E6pU",
        "outputId": "992f5bd0-f212-49b5-b537-04676df4f1e0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'food101' in TensorFlow Datasets: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_data, test_data), ds_info = tfds.load(name=\"food101\", # target dataset to get from TFDS\n",
        "                                             split=[\"train\", \"validation\"], # what splits of data should we get? note: not all datasets have train, valid, test\n",
        "                                             shuffle_files=True, # shuffle files on download?\n",
        "                                             as_supervised=True, # download data in tuple format (sample, label), e.g. (image, label)\n",
        "                                             with_info=True) # include dataset metadata? if so, tfds.load() returns tuple (data, ds_info)"
      ],
      "metadata": {
        "id": "_o4_rkhQG2pJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgA3C2DyK1-4",
        "outputId": "45cb42ef-8233-45eb-c519-80e0f9ab861a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='food101',\n",
              "    full_name='food101/2.0.0',\n",
              "    description=\"\"\"\n",
              "    This dataset consists of 101 food categories, with 101'000 images. For each class, 250 manually reviewed test images are provided as well as 750 training images. On purpose, the training images were not cleaned, and thus still contain some amount of noise. This comes mostly in the form of intense colors and sometimes wrong labels. All images were rescaled to have a maximum side length of 512 pixels.\n",
              "    \"\"\",\n",
              "    homepage='https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/',\n",
              "    data_path='/root/tensorflow_datasets/food101/2.0.0',\n",
              "    file_format=tfrecord,\n",
              "    download_size=4.65 GiB,\n",
              "    dataset_size=4.77 GiB,\n",
              "    features=FeaturesDict({\n",
              "        'image': Image(shape=(None, None, 3), dtype=uint8),\n",
              "        'label': ClassLabel(shape=(), dtype=int64, num_classes=101),\n",
              "    }),\n",
              "    supervised_keys=('image', 'label'),\n",
              "    disable_shuffling=False,\n",
              "    splits={\n",
              "        'train': <SplitInfo num_examples=75750, num_shards=32>,\n",
              "        'validation': <SplitInfo num_examples=25250, num_shards=16>,\n",
              "    },\n",
              "    citation=\"\"\"@inproceedings{bossard14,\n",
              "      title = {Food-101 -- Mining Discriminative Components with Random Forests},\n",
              "      author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},\n",
              "      booktitle = {European Conference on Computer Vision},\n",
              "      year = {2014}\n",
              "    }\"\"\",\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_info.features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6PuXnZEMHhD",
        "outputId": "68fcbb20-0b29-46d5-e45a-7fcb3334fe08"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FeaturesDict({\n",
              "    'image': Image(shape=(None, None, 3), dtype=uint8),\n",
              "    'label': ClassLabel(shape=(), dtype=int64, num_classes=101),\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ds_info.features['label'].names\n",
        "class_names[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q_6md-PMK0k",
        "outputId": "50f525ad-a140-4247-9ff8-c9f800acf295"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_info.features['image']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxf7fJxwMiEv",
        "outputId": "a7b75c77-5cf8-4768-fbc2-bc2b540d939e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Image(shape=(None, None, 3), dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Resize(img, label, size=224):\n",
        "\n",
        "  image = tf.image.resize(img, [size,size])\n",
        "  return tf.cast(image, tf.float32), label"
      ],
      "metadata": {
        "id": "3hIV2v2UM1Qv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map preprocessing function to training data (and paralellize)\n",
        "train_data = train_data.map(map_func=Resize, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# Shuffle train_data and turn it into batches and prefetch it (load it faster)\n",
        "train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Map prepreprocessing function to test data\n",
        "test_data = test_data.map(Resize, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# Turn test data into batches (don't need to shuffle)\n",
        "test_data = test_data.batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "vUVrSl7FNXV9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8z2aCy8N1OM",
        "outputId": "617602c9-ba0f-4a01-b836-f18c87a2e186"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>,\n",
              " <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn on mixed precision training\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy(policy=\"mixed_float16\") # set global policy to mixed precision"
      ],
      "metadata": {
        "id": "RfuWIxVyN2r-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mixed_precision.global_policy() # should output \"mixed_float16\" (if your GPU is compatible with mixed precision)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATq946_zOGlJ",
        "outputId": "344a8797-6698-480a-a58a-43f18d7fc0f3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Policy \"mixed_float16\">"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers as L\n",
        "\n",
        "def create_model():\n",
        "\n",
        "  input_shape = (224, 224, 3)\n",
        "\n",
        "  base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "  base_model.trainable = False\n",
        "\n",
        "  Inputs = L.Input(shape = input_shape)\n",
        "\n",
        "  x = base_model (Inputs, training=False)\n",
        "  x = L.GlobalAveragePooling2D()(x)\n",
        "  x = L.Dense(len(class_names))(x)\n",
        "  Outputs = L.Activation(\"softmax\", dtype=tf.float32)(x)\n",
        "\n",
        "  model_1 = tf.keras.Model(Inputs, Outputs)\n",
        "  return model_1\n",
        "\n",
        "model_1 = create_model()\n",
        "model_1.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                      optimizer=tf.keras.optimizers.Adam(),\n",
        "                      metrics=[\"accuracy\"])\n",
        "\n",
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRZ0b6T8OLNe",
        "outputId": "d21de861-1419-4f4e-8d9b-8319319f8f74"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " efficientnetb0 (Functional  (None, None, None, 1280   4049571   \n",
            " )                           )                                   \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 1280)              0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 101)               129381    \n",
            "                                                                 \n",
            " activation (Activation)     (None, 101)               0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4178952 (15.94 MB)\n",
            "Trainable params: 129381 (505.39 KB)\n",
            "Non-trainable params: 4049571 (15.45 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorBoard callback (already have \"create_tensorboard_callback()\" from a previous notebook)\n",
        "from helper_functions import create_tensorboard_callback\n",
        "\n",
        "# Create ModelCheckpoint callback to save model's progress\n",
        "checkpoint_path = \"model_checkpoints/cp.ckpt\" # saving weights requires \".ckpt\" extension\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                      monitor=\"val_accuracy\", # save the model weights with best validation accuracy\n",
        "                                                      save_best_only=True, # only save the best weights\n",
        "                                                      save_weights_only=True, # only save model weights (not whole model)\n",
        "                                                      verbose=0) # don't print out whether or not model is being saved"
      ],
      "metadata": {
        "id": "A16IEGlDYupw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn off all warnings except for errors\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "'''# Fit the model with callbacks\n",
        "history_101_food_classes_feature_extract = model_1.fit(train_data,\n",
        "                                                     epochs=3,\n",
        "                                                     steps_per_epoch=len(train_data),\n",
        "                                                     validation_data=test_data,\n",
        "                                                     validation_steps=int(0.15 * len(test_data)),\n",
        "                                                     callbacks=[create_tensorboard_callback(\"training_logs\",\n",
        "                                                                                            \"efficientnetb0_101_classes_all_data_feature_extract\"),\n",
        "                                                                model_checkpoint])'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "FGOvKx_JRjOE",
        "outputId": "34b292e0-b7dc-4525-cb51-cc9aa2d444af"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Fit the model with callbacks\\nhistory_101_food_classes_feature_extract = model_1.fit(train_data, \\n                                                     epochs=3,\\n                                                     steps_per_epoch=len(train_data),\\n                                                     validation_data=test_data,\\n                                                     validation_steps=int(0.15 * len(test_data)),\\n                                                     callbacks=[create_tensorboard_callback(\"training_logs\", \\n                                                                                            \"efficientnetb0_101_classes_all_data_feature_extract\"),\\n                                                                model_checkpoint])'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''results_feature_extract_model = model_1.evaluate(test_data)\n",
        "results_feature_extract_model'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "sk7QDXq3pPDz",
        "outputId": "287105d6-2f62-4aff-d191-85741ddad452"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'results_feature_extract_model = model_1.evaluate(test_data)\\nresults_feature_extract_model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''created_model = create_model()\n",
        "created_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                      optimizer=tf.keras.optimizers.Adam(),\n",
        "                      metrics=[\"accuracy\"])\n",
        "\n",
        "# Load the saved weights\n",
        "created_model.load_weights(checkpoint_path)\n",
        "\n",
        "# 4. Evaluate the model with loaded weights\n",
        "results_created_model_with_loaded_weights = created_model.evaluate(test_data)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "iQEKA0EXpc2_",
        "outputId": "7369bc96-5281-4357-f7fe-0a4e8af10ee2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'created_model = create_model()\\ncreated_model.compile(loss=\"sparse_categorical_crossentropy\",\\n                      optimizer=tf.keras.optimizers.Adam(),\\n                      metrics=[\"accuracy\"])\\n\\n# Load the saved weights\\ncreated_model.load_weights(checkpoint_path)\\n\\n# 4. Evaluate the model with loaded weights\\nresults_created_model_with_loaded_weights = created_model.evaluate(test_data)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# assert np.isclose(results_feature_extract_model, results_created_model_with_loaded_weights).all()"
      ],
      "metadata": {
        "id": "7rfJ-rjEpd7E"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -U tensorflow==2.13.0"
      ],
      "metadata": {
        "id": "5jYe2lfgqjKF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save_dir = \"07_efficientnetb0_feature_extract_model_mixed_precision\"\n",
        "# model_1.save(save_dir)"
      ],
      "metadata": {
        "id": "pY6GO4f-pxRt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load model previously saved above\n",
        "# loaded_saved_model = tf.keras.models.load_model(save_dir)"
      ],
      "metadata": {
        "id": "-jmbKS1lpyHi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Check the layers in the base model and see what dtype policy they're using\n",
        "# for layer in loaded_saved_model.layers[1].layers[:20]: # check only the first 20 layers to save output space\n",
        "#     print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
      ],
      "metadata": {
        "id": "wqL2zz6sp7VL"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in model_1.layers:\n",
        "    layer.trainable = True # set all layers to trainable"
      ],
      "metadata": {
        "id": "ddxbs6NHQfkg"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in model_1.layers[1].layers[:20]:\n",
        "    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaTv9xYOQnnE",
        "outputId": "daacae63-2a0f-455f-ae02-5b1be558bb5a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_1 True float32 <Policy \"float32\">\n",
            "rescaling True float32 <Policy \"mixed_float16\">\n",
            "normalization True float32 <Policy \"mixed_float16\">\n",
            "rescaling_1 True float32 <Policy \"mixed_float16\">\n",
            "stem_conv_pad True float32 <Policy \"mixed_float16\">\n",
            "stem_conv True float32 <Policy \"mixed_float16\">\n",
            "stem_bn True float32 <Policy \"mixed_float16\">\n",
            "stem_activation True float32 <Policy \"mixed_float16\">\n",
            "block1a_dwconv True float32 <Policy \"mixed_float16\">\n",
            "block1a_bn True float32 <Policy \"mixed_float16\">\n",
            "block1a_activation True float32 <Policy \"mixed_float16\">\n",
            "block1a_se_squeeze True float32 <Policy \"mixed_float16\">\n",
            "block1a_se_reshape True float32 <Policy \"mixed_float16\">\n",
            "block1a_se_reduce True float32 <Policy \"mixed_float16\">\n",
            "block1a_se_expand True float32 <Policy \"mixed_float16\">\n",
            "block1a_se_excite True float32 <Policy \"mixed_float16\">\n",
            "block1a_project_conv True float32 <Policy \"mixed_float16\">\n",
            "block1a_project_bn True float32 <Policy \"mixed_float16\">\n",
            "block2a_expand_conv True float32 <Policy \"mixed_float16\">\n",
            "block2a_expand_bn True float32 <Policy \"mixed_float16\">\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", # watch the val loss metric\n",
        "                                                  patience=3) # if val loss decreases for 3 epochs in a row, stop training\n",
        "\n",
        "# Create ModelCheckpoint callback to save best model during fine-tuning\n",
        "checkpoint_path = \"fine_tune_checkpoints/\"\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                      save_best_only=True,\n",
        "                                                      monitor=\"val_loss\")"
      ],
      "metadata": {
        "id": "MkMrE6E-QBO7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating learning rate reduction callback\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
        "                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n",
        "                                                 patience=2,\n",
        "                                                 verbose=1, # print out when learning rate goes down\n",
        "                                                 min_lr=1e-7)"
      ],
      "metadata": {
        "id": "LDVwUcfvQuyV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.compile(loss=\"sparse_categorical_crossentropy\", # sparse_categorical_crossentropy for labels that are *not* one-hot\n",
        "                        optimizer=tf.keras.optimizers.Adam(0.0001), # 10x lower learning rate than the default\n",
        "                        metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "45OCY-FDRAvZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -U tensorflow==2.13.0"
      ],
      "metadata": {
        "id": "hw0DMYHwE95k"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_101_food_classes_all_data_fine_tune = model_1.fit(train_data,\n",
        "                                                        epochs=100, # fine-tune for a maximum of 100 epochs\n",
        "                                                        steps_per_epoch=len(train_data),\n",
        "                                                        validation_data=test_data,\n",
        "                                                        validation_steps=int(0.15 * len(test_data)), # validation during training on 15% of test data\n",
        "                                                        callbacks=[create_tensorboard_callback(\"training_logs\", \"efficientb0_101_classes_all_data_fine_tuning\"), # track the model training logs\n",
        "                                                                   model_checkpoint, # save only the best model during training\n",
        "                                                                   early_stopping, # stop model after X epochs of no improvements\n",
        "                                                                   reduce_lr]) # reduce the learning rate after X epochs of no improvement"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVgxYz8TRKmR",
        "outputId": "672e6737-c673-46e9-c3a0-d3580f7ad28d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving TensorBoard log files to: training_logs/efficientb0_101_classes_all_data_fine_tuning/20230905-160509\n",
            "Epoch 1/100\n",
            "2368/2368 [==============================] - 464s 171ms/step - loss: 1.6037 - accuracy: 0.5931 - val_loss: 0.8820 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "2368/2368 [==============================] - 409s 171ms/step - loss: 0.8419 - accuracy: 0.7704 - val_loss: 0.7362 - val_accuracy: 0.7905 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "2368/2368 [==============================] - 372s 156ms/step - loss: 0.5336 - accuracy: 0.8500 - val_loss: 0.7519 - val_accuracy: 0.8006 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "2368/2368 [==============================] - ETA: 0s - loss: 0.3086 - accuracy: 0.9085\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "2368/2368 [==============================] - 374s 157ms/step - loss: 0.3086 - accuracy: 0.9085 - val_loss: 0.7913 - val_accuracy: 0.8006 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "2368/2368 [==============================] - 376s 158ms/step - loss: 0.0747 - accuracy: 0.9805 - val_loss: 0.8424 - val_accuracy: 0.8212 - lr: 2.0000e-05\n"
          ]
        }
      ]
    }
  ]
}