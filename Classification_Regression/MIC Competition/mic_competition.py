# -*- coding: utf-8 -*-
"""MIC_Competition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/shravan-18/Machine-Learning-Projects/blob/main/Classification_Regression/MIC%20Competition/MIC_Competition.ipynb

# **Hell Week MIC**
## **Only the Strong will Survive!!!**

### **My submission for the MIC Kaggle Competition - Shravan**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

train_data = pd.read_csv('/kaggle/input/mic-dataset/train.csv')
train_data.head(10)

train_data_copy = train_data

train_data.shape

train_data.info()

train_data.describe()

train_data.isna().sum()

# Check total number of rows which contain atleast 1 NULL or NAN value
train_data.isna().any(axis=1).sum()

# train_data = train_data_copy

# Removing redundant/unnecessary columns
red_cols = ['ph_no', 'credit_card_number', 'email', 'cvv', 'url', 'emoji', 'name']
train_data = train_data.drop(red_cols, axis=1)

# Check the data's shape and number of null values in each column..
print(train_data.shape)
train_data.isnull().sum()

"""# **Data Preprocessing**"""

# Filling NULL Values in each column using mode of that column..
for column in train_data.columns:
    train_data[column].fillna(train_data[column].mode()[0], inplace=True)

# Checking if the values in the a column have same prefix (and suffix, but not checked) throughout..
c=0
v=0
x = train_data['vF2is'][0][:6]
for i in range(len(train_data)):
    try:
        if train_data['vF2is'][i][:6]==x:
            c += 1
        else:
            print(train_data['vF2is'][i][:6])
    except KeyError:
        v += 1
print(c, v)

for i, col in enumerate(train_data.columns[4:]):
    print(f"{i}  \t{train_data[col][0]}  \t{train_data[col][1]}")

# We can see that there is a set of useless symbols in each column, which hold no meaning.
# Let's extract the useful part alone and replace the original values with useful new ones.

def extract_useful_part(a, b, check):
    # Find the common prefix
    common_prefix = ''
    for i in range(min(len(a), len(b))):
        if a[i] == b[i]:
            common_prefix += a[i]
        else:
            break

    # Find the common suffix
    a_rev, b_rev = a[::-1], b[::-1]
    common_suffix = ''
    for i in range(min(len(a_rev), len(b_rev))):
        if a_rev[i] == b_rev[i]:
            common_suffix = a_rev[i] + common_suffix
        else:
            break

    if check==0:
        return common_prefix + common_suffix
    else:
        return float(b[len(common_prefix):-len(common_suffix) if common_suffix else None])

ref_strings = {}
for col in train_data.columns[3:-1]:
    ref_strings[col] = extract_useful_part(train_data[col][0], train_data[col][1], 0)

train_data.columns[3:]

# Apply the above function to each column's values to extract the useful part alone
for column in train_data.columns[3:-1]:
    a = ref_strings[column]
    print(column)
    train_data[column] = train_data[column].apply(lambda b: extract_useful_part(a, b, 1))

train_data.head()

# Find out the number of unique values under country
len(train_data['country'].unique())

# Find out the number of unique values under job
len(train_data['job'].unique())

# Find out the number of unique values under state - target column
print(len(train_data['state'].unique()))
train_data['state'].unique()

from sklearn.feature_extraction import FeatureHasher

df = pd.DataFrame()
df['hashed_job'] = pd.Categorical(train_data['job']).codes
df.head()

from category_encoders import BinaryEncoder

encoder = BinaryEncoder(cols=['country'])
df_encoded = encoder.fit_transform(train_data)

print(df_encoded.columns)
print()
df_encoded.head()

df_encoded = df_encoded.drop('job', axis=1)
train_data = pd.concat([df_encoded, df], axis=1)
train_data.head()

import seaborn

# Plot a Heatmap of the columns, to check their correlations
plt.figure(figsize=(18,18))
seaborn.heatmap(train_data.drop(['UID','state'], axis=1).corr().abs(), cmap="YlGnBu", annot=False)
plt.show()

# Set the threshold for correlation
threshold = 0.9

# Find highly correlated columns
correlation_matrix = train_data.drop(['UID','state'], axis=1).corr()
highly_correlated_cols = set()
cols = correlation_matrix.columns

for i in range(len(cols)):
    for j in range(i):
        if correlation_matrix.iloc[i, j] >= threshold:
            colname = correlation_matrix.columns[i]
            highly_correlated_cols.add(colname)

print(highly_correlated_cols)

train_data = train_data.drop(highly_correlated_cols, axis=1)
train_data.shape

"""# **Train Test Split and Scale the Data**"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

X = train_data.drop(['UID', 'state'], axis=1)
y = train_data['state']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create an instance of MinMaxScaler
scaler = MinMaxScaler()

# Fit the scaler on your training data
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

"""# **Machine Learning Models**

### Importing the necessary models
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from sklearn.ensemble import AdaBoostClassifier
from lightgbm import LGBMClassifier
from sklearn.linear_model import LogisticRegression

# Define a list of classifiers to test
classifiers = {
    'Logistic Regression': LogisticRegression(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'Support Vector Machine': SVC(),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Naive Bayes': GaussianNB(),
    'Light GBM': LGBMClassifier(),
    'XG Boost': XGBClassifier(),
    'CatBoost': CatBoostClassifier()
}

# Train and evaluate each classifier
for name, clf in classifiers.items():
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

    report = classification_report(y_test, y_pred)

    print(f'Classifier: {name}')
    print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')
    print(f'Precision: {precision_score(y_test, y_pred, average="macro"):.2f}')
    print(f'Recall: {recall_score(y_test, y_pred, average="macro"):.2f}')
    print(f'F1 Score: {f1_score(y_test, y_pred, average="macro"):.2f}')
    print(f'Classification Report:\n{report}\n')

# I feel CatBoost will be the best, so I run it alone again

CBC = CatBoostClassifier(iterations=1000, depth=6)
CBC.fit(X_train, y_train)

y_pred = CBC.predict(X_test)

report = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')
print(f'Precision: {precision_score(y_test, y_pred, average="macro"):.2f}')
print(f'Recall: {recall_score(y_test, y_pred, average="macro"):.2f}')
print(f'F1 Score: {f1_score(y_test, y_pred, average="macro"):.2f}')
print(f'Classification Report:\n{report}\n')

"""# **Working with Test Data**

### Preprocessing Pipeline
"""

# Defining a preprocessing pipeline to use finally

def test_extract_useful_part(a, b, check):
    # Find the common prefix
    common_prefix = ''
    for i in range(min(len(a), len(b))):
        if a[i] == b[i]:
            common_prefix += a[i]
        else:
            break

    # Find the common suffix
    a_rev, b_rev = a[::-1], b[::-1]
    common_suffix = ''
    for i in range(min(len(a_rev), len(b_rev))):
        if a_rev[i] == b_rev[i]:
            common_suffix = a_rev[i] + common_suffix
        else:
            break

    if check==0:
        return common_prefix + common_suffix
    else:
        return float(b[len(common_prefix):-len(common_suffix) if common_suffix else None])


def PreprocessingPipeline(df):
    # Removing redundant/unnecessary columns
    red_cols = ['ph_no', 'credit_card_number', 'cvv', 'email', 'url', 'emoji', 'name']
    df = df.drop(red_cols, axis=1)

    # Filling NULL Values in each column using mode of that column..
    for column in df.columns[1:]:
        df[column].fillna(df[column].mode()[0], inplace=True)

    ref_strings = {}
    for col in df.columns[3:]:
        ref_strings[col] = test_extract_useful_part(df[col][0], df[col][1], 0)

    # Apply the above function to each column's values to extract the useful part alone
    for column in df.columns[3:]:
        a = ref_strings[column]
        df[column] = df[column].apply(lambda b: test_extract_useful_part(a, b, 1))

    temp_df = pd.DataFrame()
    temp_df['hashed_job'] = pd.Categorical(df['job']).codes

    encoder = BinaryEncoder(cols=['country'])
    df_encoded = encoder.fit_transform(df)

    df_encoded = df_encoded.drop('job', axis=1)
    df = pd.concat([df_encoded, temp_df], axis=1)

    df = df.drop(['sRaqu', 'op6uG', 'pZijn', 'aUxm5', '7hdd4', 'dTj0P', 'WUc3c'], axis=1)

    return df

# Read the test data
test_data = pd.read_csv('/kaggle/input/mic-dataset/test.csv')
test_data.head()

"""## **Test Data Exploration**"""

test_data.isnull().sum()

test_UID = test_data['UID']
test_UID

test_data = PreprocessingPipeline(test_data)
test_data.head()

scaler = MinMaxScaler()
test_data = scaler.fit_transform(test_data.drop('UID', axis=1))

test_data.shape

"""## **Predict on the test data**"""

test_predictions = CBC.predict(test_data)
test_predictions, np.unique(test_predictions)

test_predictions.shape

# View sample submission format
sample_sub = pd.read_csv('/kaggle/input/mic-dataset/sample_submission.csv')
sample_sub.head()

test_state = pd.DataFrame(test_predictions, columns=['state'])
test_state.head()

submission = pd.concat([test_UID, test_state], axis=1, ignore_index=True)
submission.head()

submission.columns

submission.shape

# import os
# os.remove('/kaggle/working/submissions.csv')

# Save the test results as a CSV file
submission.to_csv('/kaggle/working/submissions.csv')