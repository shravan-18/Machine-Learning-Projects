# -*- coding: utf-8 -*-
"""FoodClasses_101.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/shravan-18/Machine-Learning-Projects/blob/main/Computer-Vision/Food%20Classes%20-%20101/FoodClasses_101.ipynb
"""

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py

import tensorflow as tf

tf.__version__

from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys

import tensorflow as tf

# Get TensorFlow Datasets
import tensorflow_datasets as tfds

# Get all available datasets in TFDS
datasets_list = tfds.list_builders()

# Set our target dataset and see if it exists
target_dataset = "food101"
print(f"'{target_dataset}' in TensorFlow Datasets: {target_dataset in datasets_list}")

(train_data, test_data), ds_info = tfds.load(name="food101", # target dataset to get from TFDS
                                             split=["train", "validation"], # what splits of data should we get? note: not all datasets have train, valid, test
                                             shuffle_files=True, # shuffle files on download?
                                             as_supervised=True, # download data in tuple format (sample, label), e.g. (image, label)
                                             with_info=True) # include dataset metadata? if so, tfds.load() returns tuple (data, ds_info)

ds_info

ds_info.features

class_names = ds_info.features['label'].names
class_names[:5]

ds_info.features['image']

def Resize(img, label, size=224):

  image = tf.image.resize(img, [size,size])
  return tf.cast(image, tf.float32), label

# Map preprocessing function to training data (and paralellize)
train_data = train_data.map(map_func=Resize, num_parallel_calls=tf.data.AUTOTUNE)
# Shuffle train_data and turn it into batches and prefetch it (load it faster)
train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)

# Map prepreprocessing function to test data
test_data = test_data.map(Resize, num_parallel_calls=tf.data.AUTOTUNE)
# Turn test data into batches (don't need to shuffle)
test_data = test_data.batch(32).prefetch(tf.data.AUTOTUNE)

train_data, test_data

# Turn on mixed precision training
from tensorflow.keras import mixed_precision
mixed_precision.set_global_policy(policy="mixed_float16") # set global policy to mixed precision

mixed_precision.global_policy() # should output "mixed_float16" (if your GPU is compatible with mixed precision)

from tensorflow.keras import layers as L

def create_model():

  input_shape = (224, 224, 3)

  base_model = tf.keras.applications.EfficientNetB0(include_top=False)
  base_model.trainable = False

  Inputs = L.Input(shape = input_shape)

  x = base_model (Inputs, training=False)
  x = L.GlobalAveragePooling2D()(x)
  x = L.Dense(len(class_names))(x)
  Outputs = L.Activation("softmax", dtype=tf.float32)(x)

  model_1 = tf.keras.Model(Inputs, Outputs)
  return model_1

model_1 = create_model()
model_1.compile(loss="sparse_categorical_crossentropy",
                      optimizer=tf.keras.optimizers.Adam(),
                      metrics=["accuracy"])

model_1.summary()

# Create TensorBoard callback (already have "create_tensorboard_callback()" from a previous notebook)
from helper_functions import create_tensorboard_callback

# Create ModelCheckpoint callback to save model's progress
checkpoint_path = "model_checkpoints/cp.ckpt" # saving weights requires ".ckpt" extension
model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,
                                                      monitor="val_accuracy", # save the model weights with best validation accuracy
                                                      save_best_only=True, # only save the best weights
                                                      save_weights_only=True, # only save model weights (not whole model)
                                                      verbose=0) # don't print out whether or not model is being saved

# Turn off all warnings except for errors
tf.get_logger().setLevel('ERROR')

'''# Fit the model with callbacks
history_101_food_classes_feature_extract = model_1.fit(train_data,
                                                     epochs=3,
                                                     steps_per_epoch=len(train_data),
                                                     validation_data=test_data,
                                                     validation_steps=int(0.15 * len(test_data)),
                                                     callbacks=[create_tensorboard_callback("training_logs",
                                                                                            "efficientnetb0_101_classes_all_data_feature_extract"),
                                                                model_checkpoint])'''

'''results_feature_extract_model = model_1.evaluate(test_data)
results_feature_extract_model'''

'''created_model = create_model()
created_model.compile(loss="sparse_categorical_crossentropy",
                      optimizer=tf.keras.optimizers.Adam(),
                      metrics=["accuracy"])

# Load the saved weights
created_model.load_weights(checkpoint_path)

# 4. Evaluate the model with loaded weights
results_created_model_with_loaded_weights = created_model.evaluate(test_data)'''

# import numpy as np
# assert np.isclose(results_feature_extract_model, results_created_model_with_loaded_weights).all()

# pip install -U tensorflow==2.13.0

# save_dir = "07_efficientnetb0_feature_extract_model_mixed_precision"
# model_1.save(save_dir)

# # Load model previously saved above
# loaded_saved_model = tf.keras.models.load_model(save_dir)

# # Check the layers in the base model and see what dtype policy they're using
# for layer in loaded_saved_model.layers[1].layers[:20]: # check only the first 20 layers to save output space
#     print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)

for layer in model_1.layers:
    layer.trainable = True # set all layers to trainable

for layer in model_1.layers[1].layers[:20]:
    print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy)

# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs
early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_loss", # watch the val loss metric
                                                  patience=3) # if val loss decreases for 3 epochs in a row, stop training

# Create ModelCheckpoint callback to save best model during fine-tuning
checkpoint_path = "fine_tune_checkpoints/"
model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,
                                                      save_best_only=True,
                                                      monitor="val_loss")

# Creating learning rate reduction callback
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor="val_loss",
                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)
                                                 patience=2,
                                                 verbose=1, # print out when learning rate goes down
                                                 min_lr=1e-7)

model_1.compile(loss="sparse_categorical_crossentropy", # sparse_categorical_crossentropy for labels that are *not* one-hot
                        optimizer=tf.keras.optimizers.Adam(0.0001), # 10x lower learning rate than the default
                        metrics=["accuracy"])

# pip install -U tensorflow==2.13.0

history_101_food_classes_all_data_fine_tune = model_1.fit(train_data,
                                                        epochs=100, # fine-tune for a maximum of 100 epochs
                                                        steps_per_epoch=len(train_data),
                                                        validation_data=test_data,
                                                        validation_steps=int(0.15 * len(test_data)), # validation during training on 15% of test data
                                                        callbacks=[create_tensorboard_callback("training_logs", "efficientb0_101_classes_all_data_fine_tuning"), # track the model training logs
                                                                   model_checkpoint, # save only the best model during training
                                                                   early_stopping, # stop model after X epochs of no improvements
                                                                   reduce_lr]) # reduce the learning rate after X epochs of no improvement